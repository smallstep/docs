---
title: Smallstep Platform Installation - AWS
html_title: Smallstep Platform Installation - AWS
description: Learn how to configure your AWS infrastructure and install your run anywhere deployment
---


Smallstep's `run anywhere` deployments require a certain set of infrastructure on which to run. Details about these requirements are listed throughout this guide. We provide a Terraform module if you would like to use our code to stand up your resources, but you are entirely welcome to simply use it as a reference guide instead.

## Setting up AWS Infrastructure

### Dependencies

- [AWS CLI](https://aws.amazon.com/cli/)
- [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
- [KOTS](https://kots.io/kots-cli/getting-started/#how-to-install)
- [linkerd](https://linkerd.io/2.10/getting-started/#step-1-install-the-cli)
- [step](https://smallstep.com/docs/step-cli/installation)

### AWS CLI

All steps involving the setup of your AWS Infrastructure use the AWS CLI at some point. Make sure that you are logged in to your account on the command line with your [configuration](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) pointing at the right account.

### Terraform

If you already have a Terraform environment set up for your account, add the [run-anywhere-terraform](https://github.com/smallstep/run-anywhere-terraform/tree/main/aws) module to your configuration and follow the steps listed in its README. They are also listed as follows:

#### Secret management

Terraform will need some secrets for various pieces of your infrastructure. Some of these secrets must be manually entered and others will be auto-generated. All secrets are stored in AWS SecretsManager, encrypted by AWS KMS, and referenced by the Terraform state. Terraform will also automatically apply these secrets to your kubernetes cluster where needed.

On first apply, make sure to pass in the values of the following two variables to create the secrets for your private issuer password and SMTP password. Our recommendation is creating two high-level variables with a default value of an empty string to pass into the module block; subsequently, you can pass in the actual secrets during your first Terraform apply of the module. Both variables are marked "secret" in Terraform to avoid leaking them in Terraform's responses on the command line, and we recommend passing the command line `HISTCONTROL=ignorespace` before running your apply to prevent leaking secrets into your session history. (If you are using a YubiHSM2 and have set the value of `hsm_enabled = true`, also pass in the HSM PIN code in hexadecimal and password to variable `yubihsm_pin`. For example, authentication key id `0x0001` with password `password` would follow the form: -var yubihsm_pin="0001password")

You may instead pass in these values directly to the module block, but the above method will prevent these secrets from being written to your source control. All related resources are configured to ignore changes, so it won't matter that these values will not be passed in for subsequent Terraform applies.

Once the module has been set up, you should confirm each secret's value in the SecretsManager console. If incorrect, you can fix the secret directly in the console without disrupting the Terraform module.

After completion, Terraform will have stood up and configured an RDS Aurora PostgreSQL cluster (with lambda attached), an EKS cluster, a Redis instance, an Elastic IP (later used to create an NLB), DNS resources, and all secrets stored in SecretsManager. Additionally, it will have tagged all subnets involved to allow EKS to attach to the private subnets and our NLB to attach to the public subnets.

#### Example module intantiation

```terraform
variable "private_issuer_password" {
  default     = ""
  description = "Private issuer password used for the `run anywhere` deployment, set during first module apply and left blank otherwise."
  type        = string
  sensitive   = true
}

variable "smtp_password" {
  default     = ""
  description = "SMTP password used for the `run anywhere` deployment, set during first module apply and left blank otherwise."
  type        = string
  sensitive   = true
}

variable "yubihsm_pin" {
  default     = ""
  description = "YubiHSM PIN followed by password for the `run anywhere` deployment, set during first module apply and left blank otherwise."
  type        = string
  sensitive   = true
}

module "run_anywhere" {
  source = "github.com/smallstep/run-anywhere-terraform.git//aws?ref=v1.0.0"

  base_domain             = "your_domain.com"
  default_name            = "smallstep-prod"
  private_issuer_password = var.private_issuer_password
  region                  = "us-west-1"
  smtp_password           = var.smtp_password
  subnets_public          = ["subnet-abskd939", "subnet-283kdjjd9"]    
  subnets_private         = ["subnet-d7ddd333b3", "subnet-abscd303"]    
  yubihsm_enabled         = true
  yubihsm_pin             = var.yubihsm_pin
}
```

#### Initialize and apply

```shell
terraform init
HISTCONTROL=ignorespace
terraform apply -var private_issuer_password="${private_issuer_password}" -var smtp_password="${smtp_password}" -var yubihsm_pin="${yubihsm_pin}"
```

### DNS

(Not required if you manage your Domain with Route53)

Refer to the AWS Console, Route53 ⇒ Your Hosted Zone to retrieve the list of nameservers for your AWS DNS configuration.

In the your base domain DNS provider's records, add an `NS` record to delegate resolution of the subdomain to the DNS nameservers.

For example, an `NS` record for `smallstep.basedomain.company.com` may contain the following:

```
ns-1111.awsdns-11.org.
ns-222.awsdns-22.co.uk.
ns-123.awsdns-45.net.
ns-456.awsdns-78.com.
```

## Platform

### AWS `kubectl` context

To interact with the EKS cluster, you'll need to configure a local `kubectl` context. `aws` CLI can do this for you. They provide a comprehensive [guide](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-connection/) on how to gain `kubectl` access to the cluster.

> Your AWS profile will be the only IAM entity with access to the cluster initially. You will have the highest level of access by default. To add subsequent users and map subsequent permissions internal to the EKS cluster, follow the AWS [guide](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html) on how to update the `configmap` with your desired settings.
> 

### **Linkerd**

Smallstep services currently use Linkerd for some internal load balancing needs. Install it manually with a long lived certificate. Linkerd comes with a default certificate with a lifetime of 1 year; we don’t want our CA to become useless in 1 year, so this step is necessary. However, we may eventually remove the dependency on Linkerd.

First, create a root CA cert and key:

```bash
step certificate create root.linkerd.cluster.local ca.crt ca.key \
  --profile root-ca --no-password --insecure --not-after=87600h

```

Use the CA to issue an identity certificate for linkerd:

```bash
step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \
  --profile intermediate-ca --not-after 87600h --no-password --insecure \
  --ca ca.crt --ca-key ca.key

```

Install linkerd, providing the files from the previous commands:

```bash
kubectl config use-context <your context>

linkerd install \
  --identity-trust-anchors-file ca.crt \
  --identity-issuer-certificate-file issuer.crt \
  --identity-issuer-key-file issuer.key \
  | kubectl apply -f -
```

Shred the key material:

```bash
shred -uv ca.key issuer.key
```

## Secrets

This project uses [Kubernetes secrets](https://kubernetes.io/docs/concepts/configuration/secret/) internal to the EKS cluster to manage our passwords. You don’t have to do anything to set these up, as Terraform has already done so for you.

### Smallstep

Begin the Smallstep platform installation process (powered by KOTS). Install into the `smallstep` namespace.

```bash
kubectl config use-context <your context>
kubectl kots install smallstep/onboarding
```

### Team and Authority Configuration

Exec into the a `admin-tools` pod to run configuration tooling:

```bash
kubectl exec -it -n smallstep deploy/admin-tools -c admin-tools -- bash
```

**Create a new team**

Make sure the team slug matches the slug used in your smallstep installation.

```bash
create-team
```

**Create a new authority**

Use the convention `ssh.<team-slug>.ca.<base-domain>` for the domain:

```bash
create-authority --ssh

manage-provisioners --ssh --type SSHPOP --name "SSH POP" --authority <authority-id> add

refresh-authority <authority-id>
```

**Add OIDC provisioner**

```bash
manage-provisioners --ssh --type OIDC --name "azuread" --listen-address 127.0.0.1:10000 --client-id <client-id> --client-secret <client-secret> --configuration-endpoint <configuraiton-endpoint> --domain <domain>  --authority <authority-id> --tenant-id <azure-tenant-id> add
```

**Note:** After syncing over SCIM, don't forget to update the team SSH directory to point to the new IdP directory.

```bash
kubectl exec -it -n smallstep deploy/admin-tools -c admin-tools -- bash

$ psql folk
> update teams set ssh_directory_id = '<directory-id>' where slug = '<team-slug>';
```